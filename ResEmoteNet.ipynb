{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download dataset from Kaggle\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMqYpHAJh0vE",
        "outputId": "6547f9e2-81ab-401e-e3b4-9d6215dad14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/msambare/fer2013?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60.3M/60.3M [00:00<00:00, 172MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyzBTht5ibBF",
        "outputId": "e5559ad8-e410-43d4-dcca-9bab0e462c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion labels corresponding to the indices (0-6)\n",
        "emotion_labels = {\n",
        "    0: 'angry',\n",
        "    1: 'disgust',\n",
        "    2: 'fear',\n",
        "    3: 'happy',\n",
        "    4: 'sad',\n",
        "    5: 'surprise',\n",
        "    6: 'neutral'\n",
        "}\n",
        "\n",
        "# Modify the FolderDataset class as before, using the updated emotion_labels\n",
        "class FolderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Build the class_to_idx mapping to reflect the emotion_labels\n",
        "        self.class_to_idx = {emotion: idx for idx, emotion in emotion_labels.items()}\n",
        "\n",
        "        # Iterate over class folders and populate image paths and labels\n",
        "        for emotion_name, idx in self.class_to_idx.items():\n",
        "            class_folder = os.path.join(root_dir, emotion_name)  # Ensure folder name matches emotion label\n",
        "            if os.path.exists(class_folder):  # Check if the folder exists\n",
        "                for img_file in os.listdir(class_folder):\n",
        "                    self.image_paths.append(os.path.join(class_folder, img_file))\n",
        "                    self.labels.append(idx)  # Use the index corresponding to the emotion label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "-UWkXK2xh7iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDjnz-Zu6Kok"
      },
      "outputs": [],
      "source": [
        "# Define transforms with extended data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Load datasets using the downloaded path\n",
        "train_dir = os.path.join(path, \"train\")\n",
        "test_dir = os.path.join(path, \"test\")\n",
        "\n",
        "train_dataset = FolderDataset(train_dir, transform=transform)\n",
        "test_dataset = FolderDataset(test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a few samples in the dataset\n",
        "for i, (image, label) in enumerate(train_loader):\n",
        "    # Get the image path and label for a few samples\n",
        "    sample_image_path = train_dataset.image_paths[i]\n",
        "    sample_label = train_dataset.labels[i]\n",
        "    emotion_label = emotion_labels[sample_label]\n",
        "\n",
        "    print(f\"Image Path: {sample_image_path}\")\n",
        "    print(f\"Assigned Label (Index): {sample_label} -> Emotion: {emotion_label}\")\n",
        "\n",
        "    # Display the image (optional)\n",
        "    img = Image.open(sample_image_path)\n",
        "    img.show()\n",
        "\n",
        "    if i == 2:  # Print only the first few samples (change number for more samples)\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hipajd24irxI",
        "outputId": "00d65ec6-d849-48ae-84e1-5f0c3994e8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Path: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/train/angry/Training_45902257.jpg\n",
            "Assigned Label (Index): 0 -> Emotion: angry\n",
            "Image Path: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/train/angry/Training_79256784.jpg\n",
            "Assigned Label (Index): 0 -> Emotion: angry\n",
            "Image Path: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/train/angry/Training_68308223.jpg\n",
            "Assigned Label (Index): 0 -> Emotion: angry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SEBlock for ResEmoteNet\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# ResidualBlock for ResEmoteNet\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0),\n",
        "                nn.BatchNorm2d(out_ch)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResEmoteNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResEmoteNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se = SEBlock(256)\n",
        "\n",
        "        self.res_block1 = ResidualBlock(256, 512, stride=2)\n",
        "        self.res_block2 = ResidualBlock(512, 1024, stride=2)\n",
        "        self.res_block3 = ResidualBlock(1024, 2048, stride=2)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(2048, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc4 = nn.Linear(256, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.se(x)\n",
        "\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "        x = self.res_block3(x)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResEmoteNet().to(device)\n",
        "\n",
        "# Define criterion, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training parameters\n",
        "patience = 15\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "epoch_counter = 0\n",
        "\n",
        "num_epochs = 80\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []"
      ],
      "metadata": {
        "id": "VYHSS__6ioUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training process with validation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    test_running_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = test_running_loss / len(test_loader)\n",
        "    test_acc = test_correct / test_total\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_acc}, Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement in validation accuracy for {patience_counter} epochs.\")\n",
        "\n",
        "    if patience_counter > patience:\n",
        "        print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "QLcz4wNCj-nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Epoch': range(1, epoch_counter+1),\n",
        "    'Train Loss': train_losses,\n",
        "    'Test Loss': test_losses,\n",
        "    'Validation Loss': val_losses,\n",
        "    'Train Accuracy': train_accuracies,\n",
        "    'Test Accuracy': test_accuracies,\n",
        "    'Validation Accuracy': val_accuracies\n",
        "})\n",
        "df.to_csv('result_four4all.csv', index=False)"
      ],
      "metadata": {
        "id": "UrUw0NQw3gW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}